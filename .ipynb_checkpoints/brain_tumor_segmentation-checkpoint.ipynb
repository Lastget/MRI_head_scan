{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain MRI Segmentation\n",
    "### Data source: Decathlon 10 Challenge. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is stored in the NifTI-1 format and we will be using the NiBabel library to interact with the files. Each training sample is composed of two separate files:\n",
    "\n",
    "The first file is an image file containing a 4D array of MR image in the shape of (240, 240, 155, 4).\n",
    "\n",
    "- The first 3 dimensions are the X, Y, and Z values for each point in the 3D volume, which is commonly called a voxel.\n",
    "- The 4th dimension is the values for 4 different sequences\n",
    "    - 0: FLAIR: \"Fluid Attenuated Inversion Recovery\" (FLAIR)\n",
    "    - 1: T1w: \"T1-weighted\"\n",
    "    - 2: t1gd: \"T1-weighted with gadolinium contrast enhancement\" (T1-Gd)\n",
    "    - 3: T2w: \"T2-weighted\"\n",
    "The second file in each training example is a label file containing a 3D array with the shape of (240, 240, 155).\n",
    "\n",
    "- The integer values in this array indicate the \"label\" for each voxel in the corresponding image files:\n",
    "    - 0: background\n",
    "    - 1: edema\n",
    "    - 2: non-enhancing tumor\n",
    "    - 3: enhancing tumor\n",
    "We have access to a total of 484 training images which we will be splitting into a training (80%) and validation (20%) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "\n",
    "from tensorflow.keras import backend as K \n",
    "\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Exploring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set home directory and data directory\n",
    "HOME_DIR = \"./BraTS-Data/\"\n",
    "DATA_DIR = HOME_DIR\n",
    "\n",
    "def load_case(image_nifty_file, label_nifty_file):\n",
    "    # load the image and label file, get the image content and return a numpy array for each\n",
    "    image = np.array(nib.load(image_nifty_file).get_fdata())\n",
    "    label = np.array(nib.load(label_nifty_file).get_fdata())\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labeled_image(image, label, is_categorical=False):\n",
    "    if not is_categorical:\n",
    "        label = to_categorical(label, num_classes=4).astype(np.uint8)\n",
    "\n",
    "    image = cv2.normalize(image[:, :, :, 0], None, alpha=0, beta=255,\n",
    "                          norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F).astype(\n",
    "        np.uint8)\n",
    "\n",
    "    labeled_image = np.zeros_like(label[:, :, :, 1:])\n",
    "\n",
    "    # remove tumor part from image\n",
    "    labeled_image[:, :, :, 0] = image * (label[:, :, :, 0])\n",
    "    labeled_image[:, :, :, 1] = image * (label[:, :, :, 0])\n",
    "    labeled_image[:, :, :, 2] = image * (label[:, :, :, 0])\n",
    "\n",
    "    # color labels\n",
    "    labeled_image += label[:, :, :, 1:] * 255\n",
    "    return labeled_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now visualize an example. For this, we use a pre-defined function we have written in the util.py file that uses matplotlib to generate a summary of the image.\n",
    "\n",
    "The colors correspond to each class.\n",
    "\n",
    "- Red is edema\n",
    "- Green is a non-enhancing tumor\n",
    "- Blue is an enhancing tumor.\n",
    "Do feel free to look at this function at your own time to understand how this is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = load_case(DATA_DIR + \"imagesTr/BRATS_003.nii.gz\", DATA_DIR + \"labelsTr/BRATS_003.nii.gz\")\n",
    "image = util.get_labeled_image(image, label)\n",
    "\n",
    "util.plot_image_grid(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = load_case(DATA_DIR + \"imagesTr/BRATS_003.nii.gz\", DATA_DIR + \"labelsTr/BRATS_003.nii.gz\")\n",
    "util.visualize_data_gif(util.get_labeled_image(image, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Data Preprocessing using patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Sub-volume Sampling\n",
    "The function below takes in:\n",
    "\n",
    "- a 4D image (shape: [240, 240, 155, 4])\n",
    "- its 3D label (shape: [240, 240, 155]) arrays,\n",
    "\n",
    "The function returns:\n",
    "\n",
    "- A randomly generated sub-volume of size [160, 160, 16]\n",
    "- Its corresponding label in a 1-hot format which has the shape [3, 160, 160, 16]\n",
    "\n",
    "Additionally:\n",
    "\n",
    "1. Make sure that at most 95% of the returned patch is non-tumor regions.\n",
    "2. Given that our network expects the channels for our images to appear as the first dimension (instead of the last one in our current setting) reorder the dimensions of the image to have the channels appear as the first dimension.\n",
    "3. Reorder the dimensions of the label array to have the first dimension as the classes (instead of the last one in our current setting)\n",
    "4. Reduce the labels array dimension to only include the non-background classes (total of 3 instead of 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def get_sub_volume(image, label, \n",
    "                   orig_x = 240, orig_y = 240, orig_z = 155, \n",
    "                   output_x = 160, output_y = 160, output_z = 16,\n",
    "                   num_classes = 4, max_tries = 1000, \n",
    "                   background_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Extract random sub-volume from original images.\n",
    "\n",
    "    Args:\n",
    "        image (np.array): original image, \n",
    "            of shape (orig_x, orig_y, orig_z, num_channels)\n",
    "        label (np.array): original label. \n",
    "            labels coded using discrete values rather than\n",
    "            a separate dimension, \n",
    "            so this is of shape (orig_x, orig_y, orig_z)\n",
    "        orig_x (int): x_dim of input image\n",
    "        orig_y (int): y_dim of input image\n",
    "        orig_z (int): z_dim of input image\n",
    "        output_x (int): desired x_dim of output\n",
    "        output_y (int): desired y_dim of output\n",
    "        output_z (int): desired z_dim of output\n",
    "        num_classes (int): number of class labels\n",
    "        max_tries (int): maximum trials to do when sampling\n",
    "        background_threshold (float): limit on the fraction \n",
    "            of the sample which can be the background\n",
    "\n",
    "    returns:\n",
    "        X (np.array): sample of original image of dimension \n",
    "            (num_channels, output_x, output_y, output_z)\n",
    "        y (np.array): labels which correspond to X, of dimension \n",
    "            (num_classes, output_x, output_y, output_z)\n",
    "    \"\"\"\n",
    "    # Initialize features and labels with `None`\n",
    "    X = None\n",
    "    y = None\n",
    "\n",
    "    tries = 0\n",
    "    \n",
    "    while tries < max_tries:\n",
    "        # randomly sample sub-volume by sampling the corner voxel\n",
    "        # hint: make sure to leave enough room for the output dimensions!\n",
    "        start_x = np.random.randint(0, orig_x - output_x + 1)\n",
    "        start_y = np.random.randint(0, orig_y - output_y + 1)\n",
    "        start_z = np.random.randint(0, orig_z - output_z + 1)\n",
    "\n",
    "        # extract relevant area of label\n",
    "        y = label[start_x: start_x + output_x,\n",
    "                  start_y: start_y + output_y,\n",
    "                  start_z: start_z + output_z]\n",
    "        \n",
    "        # One-hot encode the categories.\n",
    "        # This adds a 4th dimension, 'num_classes'\n",
    "        # (output_x, output_y, output_z, num_classes)\n",
    "        y = keras.utils.to_categorical(y, num_classes= num_classes)\n",
    "\n",
    "        # compute the background ratio\n",
    "        bgrd_ratio = np.sum(y[:,:,:,0])/ (output_x*output_y*output_z)\n",
    "\n",
    "        # increment tries counter\n",
    "        tries += 1\n",
    "\n",
    "        # if background ratio is below the desired threshold,\n",
    "        # use that sub-volume.\n",
    "        # otherwise continue the loop and try another random sub-volume\n",
    "        if bgrd_ratio < background_threshold:\n",
    "\n",
    "            # make copy of the sub-volume\n",
    "            X = np.copy(image[start_x: start_x + output_x,\n",
    "                              start_y: start_y + output_y,\n",
    "                              start_z: start_z + output_z, :])\n",
    "            \n",
    "            # change dimension of X\n",
    "            # from (x_dim, y_dim, z_dim, num_channels)\n",
    "            # to (num_channels, x_dim, y_dim, z_dim)\n",
    "            X = np.moveaxis(X,3,0)\n",
    "\n",
    "            # change dimension of y\n",
    "            # from (x_dim, y_dim, z_dim, num_classes)\n",
    "            # to (num_classes, x_dim, y_dim, z_dim)\n",
    "            y = np.moveaxis(y,3,0)\n",
    "            \n",
    "            # take a subset of y that excludes the background class\n",
    "            # in the 'num_classes' dimension\n",
    "            y = y[1:, :, :, :]\n",
    "    \n",
    "            return X, y\n",
    "\n",
    "    # if we've tried max_tries number of samples\n",
    "    # Give up in order to avoid looping forever.\n",
    "    print(f\"Tried {tries} times to find a sub-volume. Giving up...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at candidate patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_case' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-30fb63ff79a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_case\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"imagesTr/BRATS_001.nii.gz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"labelsTr/BRATS_001.nii.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sub_volume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# enhancing tumor is channel 2 in the class label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# you can change indexer for y to look at different classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_case' is not defined"
     ]
    }
   ],
   "source": [
    "image, label = load_case(DATA_DIR + \"imagesTr/BRATS_001.nii.gz\", DATA_DIR + \"labelsTr/BRATS_001.nii.gz\")\n",
    "X, y = get_sub_volume(image, label)\n",
    "# enhancing tumor is channel 2 in the class label\n",
    "# you can change indexer for y to look at different classes\n",
    "util.visualize_patch(X[0, :, :, :], y[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Standardization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def standardize(image):\n",
    "    \"\"\"\n",
    "    Standardize mean and standard deviation \n",
    "        of each channel and z_dimension.\n",
    "\n",
    "    Args:\n",
    "        image (np.array): input image, \n",
    "            shape (num_channels, dim_x, dim_y, dim_z)\n",
    "\n",
    "    Returns:\n",
    "        standardized_image (np.array): standardized version of input image\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize to array of zeros, with same shape as the image\n",
    "    standardized_image = np.zeros(image.shape)\n",
    "\n",
    "    # iterate over channels\n",
    "    for c in range(image.shape[0]):\n",
    "        # iterate over the `z` dimension\n",
    "        for z in range(image.shape[3]):\n",
    "            # get a slice of the image \n",
    "            # at channel c and z-th dimension `z`\n",
    "            image_slice = image[c,:,:,z]\n",
    "\n",
    "            # subtract the mean from image_slice\n",
    "            centered = image_slice - np.mean(image_slice)\n",
    "            \n",
    "            # divide by the standard deviation (only if it is different from zero)\n",
    "            if np.std(centered) != 0:\n",
    "                centered_scaled = centered / np.std(centered)\n",
    "\n",
    "                # update  the slice of standardized image\n",
    "                # with the scaled centered and scaled image\n",
    "                standardized_image[c, :, :, z] = centered_scaled\n",
    "\n",
    "    return standardized_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = standardize(X)\n",
    "print(\"standard deviation for a slice should be 1.0\")\n",
    "print(f\"stddv for X_norm[0, :, :, 0]: {X_norm[0,:,:,0].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Build Metrics \n",
    "### 2.1 Dice Similarity Coefficient\n",
    "### 2.2 Dice Coefficient for Multiple classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred, axis=(1, 2, 3), \n",
    "                     epsilon=0.00001):\n",
    "    \"\"\"\n",
    "    Compute mean dice coefficient over all abnormality classes.\n",
    "\n",
    "    Args:\n",
    "        y_true (Tensorflow tensor): tensor of ground truth values for all classes.\n",
    "                                    shape: (num_classes, x_dim, y_dim, z_dim)\n",
    "        y_pred (Tensorflow tensor): tensor of predictions for all classes.\n",
    "                                    shape: (num_classes, x_dim, y_dim, z_dim)\n",
    "        axis (tuple): spatial axes to sum over when computing numerator and\n",
    "                      denominator of dice coefficient.\n",
    "                      Hint: pass this as the 'axis' argument to the K.sum\n",
    "                            and K.mean functions.\n",
    "        epsilon (float): small constant add to numerator and denominator to\n",
    "                        avoid divide by 0 errors.\n",
    "    Returns:\n",
    "        dice_coefficient (float): computed value of dice coefficient.     \n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    dice_numerator = K.sum(2 * y_true * y_pred, axis = axis) + epsilon\n",
    "    dice_denominator = K.sum(y_true, axis = axis) + K.sum(y_pred, axis = axis) + epsilon\n",
    "    dice_coefficient = K.mean(dice_numerator/dice_denominator)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return dice_coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class Soft Dice Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def soft_dice_loss(y_true, y_pred, axis=(1, 2, 3), \n",
    "                   epsilon=0.00001):\n",
    "    \"\"\"\n",
    "    Compute mean soft dice loss over all abnormality classes.\n",
    "\n",
    "    Args:\n",
    "        y_true (Tensorflow tensor): tensor of ground truth values for all classes.\n",
    "                                    shape: (num_classes, x_dim, y_dim, z_dim)\n",
    "        y_pred (Tensorflow tensor): tensor of soft predictions for all classes.\n",
    "                                    shape: (num_classes, x_dim, y_dim, z_dim)\n",
    "        axis (tuple): spatial axes to sum over when computing numerator and\n",
    "                      denominator in formula for dice loss.\n",
    "                      Hint: pass this as the 'axis' argument to the K.sum\n",
    "                            and K.mean functions.\n",
    "        epsilon (float): small constant added to numerator and denominator to\n",
    "                        avoid divide by 0 errors.\n",
    "    Returns:\n",
    "        dice_loss (float): computed value of dice loss.     \n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "\n",
    "    dice_numerator = K.sum(2 * y_true * y_pred, axis = axis) + epsilon\n",
    "    dice_denominator = K.sum(y_true**2, axis = axis) + K.sum(y_pred**2, axis = axis) + epsilon\n",
    "    dice_loss = 1- K.mean(dice_numerator/dice_denominator)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return dice_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create and Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = util.unet_model_3d(loss_function=soft_dice_loss, metrics=[dice_coefficient])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training on a Large Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Loading a Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell if you didn't run the training cell in section 4.1\n",
    "base_dir = HOME_DIR + \"processed/\"\n",
    "with open(base_dir + \"config.json\") as json_file:\n",
    "    config = json.load(json_file)\n",
    "# Get generators for training and validation sets\n",
    "train_generator = util.VolumeDataGenerator(config[\"train\"], base_dir + \"train/\", batch_size=3, dim=(160, 160, 16), verbose=0)\n",
    "valid_generator = util.VolumeDataGenerator(config[\"valid\"], base_dir + \"valid/\", batch_size=3, dim=(160, 160, 16), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(HOME_DIR + \"model_pretrained.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "### 4.1 Overall Performance \n",
    "### 4.2 Patch-level predictions \n",
    "Add a 'batch' dimension\n",
    "\n",
    "We can extract predictions by calling model.predict on the patch.\n",
    "\n",
    "- We'll add an images_per_batch dimension, since the predict method is written to take in batches.\n",
    "- The dimensions of the input should be (images_per_batch, num_channels, x_dim, y_dim, z_dim).\n",
    "- Use numpy.expand_dims to add a new dimension as the zero-th dimension by setting axis=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm_with_batch_dimension = np.expand_dims(X_norm, axis=0)\n",
    "patch_pred = model.predict(X_norm_with_batch_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set threshold.\n",
    "threshold = 0.5\n",
    "\n",
    "# use threshold to get hard predictions\n",
    "patch_pred[patch_pred > threshold] = 1.0\n",
    "patch_pred[patch_pred <= threshold] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity and Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6 \n",
    "def compute_class_sens_spec(pred, label, class_num):\n",
    "    \"\"\"\n",
    "    Compute sensitivity and specificity for a particular example\n",
    "    for a given class.\n",
    "\n",
    "    Args:\n",
    "        pred (np.array): binary arrary of predictions, shape is\n",
    "                         (num classes, height, width, depth).\n",
    "        label (np.array): binary array of labels, shape is\n",
    "                          (num classes, height, width, depth).\n",
    "        class_num (int): number between 0 - (num_classes -1) which says\n",
    "                         which prediction class to compute statistics\n",
    "                         for.\n",
    "\n",
    "    Returns:\n",
    "        sensitivity (float): precision for given class_num.\n",
    "        specificity (float): recall for given class_num\n",
    "    \"\"\"\n",
    "\n",
    "    # extract sub-array for specified class\n",
    "    class_pred = pred[class_num]\n",
    "    class_label = label[class_num]\n",
    "    \n",
    "    # compute:\n",
    "    \n",
    "    # true positives\n",
    "    tp = np.sum((class_pred == 1) & (class_label == 1)) \n",
    "\n",
    "    # true negatives\n",
    "    tn = np.sum((class_pred == 0) & (class_label == 0)) \n",
    "    \n",
    "    #false positives\n",
    "    fp = np.sum((class_pred == 1) & (class_label == 0)) \n",
    "    \n",
    "    # false negatives\n",
    "    fn = np.sum((class_pred == 0) & (class_label == 1)) \n",
    "\n",
    "    # compute sensitivity and specificity\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (fp + tn)\n",
    "\n",
    "    return sensitivity, specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity, specificity = compute_class_sens_spec(patch_pred[0], y, 2)\n",
    "\n",
    "print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also display the sensitivity and specificity for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sens_spec_df(pred, label):\n",
    "    patch_metrics = pd.DataFrame(\n",
    "        columns = ['Edema', \n",
    "                   'Non-Enhancing Tumor', \n",
    "                   'Enhancing Tumor'], \n",
    "        index = ['Sensitivity',\n",
    "                 'Specificity'])\n",
    "    \n",
    "    for i, class_name in enumerate(patch_metrics.columns):\n",
    "        sens, spec = compute_class_sens_spec(pred, label, i)\n",
    "        patch_metrics.loc['Sensitivity', class_name] = round(sens,4)\n",
    "        patch_metrics.loc['Specificity', class_name] = round(spec,4)\n",
    "\n",
    "    return patch_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_sens_spec_df(patch_pred[0], y)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Running on entire scans\n",
    "As of now, our model just runs on patches, but what we really want to see is our model's result on a whole MRI scan. \n",
    "\n",
    "- To do this, generate patches for the scan.\n",
    "- Then we run the model on the patches. \n",
    "- Then combine the results together to get a fully labeled MR image.\n",
    "\n",
    "The output of our model will be a 4D array with 3 probability values for each voxel in our data. \n",
    "- We then can use a threshold (which you can find by a calibration process) to decide whether or not to report a label for each voxel. \n",
    "\n",
    "We have written a function that stitches the patches together:  `predict_and_viz(image, label, model, threshold)` \n",
    "- Inputs: an image, label and model.\n",
    "- Ouputs: the model prediction over the whole image, and a visual of the ground truth and prediction. \n",
    "\n",
    "Run the following cell to see this function in action!\n",
    "\n",
    "#### Note: the prediction takes some time!\n",
    "- The first prediction will take about 7 to 8 minutes to run.\n",
    "- You can skip running this first prediction to save time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = load_case(DATA_DIR + \"imagesTr/BRATS_003.nii.gz\", DATA_DIR + \"labelsTr/BRATS_003.nii.gz\")\n",
    "pred = util.predict_and_viz(image, label, model, .5, loc=(130, 130, 77))                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check how well the predictions do\n",
    "\n",
    "We can see some of the discrepancies between the model and the ground truth visually. \n",
    "- We can also use the functions we wrote previously to compute sensitivity and specificity for each class over the whole scan.\n",
    "- First we need to format the label and prediction to match our functions expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_scan_label = keras.utils.to_categorical(label, num_classes = 4)\n",
    "whole_scan_pred = pred\n",
    "\n",
    "# move axis to match shape expected in functions\n",
    "whole_scan_label = np.moveaxis(whole_scan_label, 3 ,0)[1:4]\n",
    "whole_scan_pred = np.moveaxis(whole_scan_pred, 3, 0)[1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute sensitivity and specificity for each class just like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_scan_df = get_sens_spec_df(whole_scan_pred, whole_scan_label)\n",
    "\n",
    "print(whole_scan_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python38264bitvenvvenvc5cb0a23e51649d1ab36ea679ef42d5c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
